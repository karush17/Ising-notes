---
title: An Introduction to Ising Models as Computational Frameworks
date: 2020-10-03
permalink: /posts/2012/08/blog-post-22/
author_profile: false
tags:
  - Reinforcement Learning
  - Deep Learning
---


For long, Deep Learning has been motivated by the design and implementation of physics-based methods such as Boltzmann Machines and energy-based distributions for probabilistic estimations. This blog post takes the idea of physical implementations in deep learning a step further and introduces Ising models. Ising models have been the fundamental aspect of computations and approximations in the machine learning community. In fact, the famous neural network being used for majority of machine learning applications is itself a specific case of an Ising model carrying out probabilistic inference using a boltzmann distribution (also known as the softmax activation in the machine learning community). We will look at the need for adopting Ising models in the machine learning community and their capabilities from a computational perspective. Additionally, the later sections of the post will dive deeper into the working and operation of the model along with its various components. 

<h3>Why the Ising Model?</h3>
The early 1920s saw physicists working on lattice structures which were intrinsically difficult to solve. These structures consisted of inherent computational properties and their dynamic behavior in different phases opened multiple questions about their physical and chemical characteristics. Naturally, solving these lattices became of interest to physicists who originally worked on lower dimensional models. Ernst Ising (below), a german physicist, solved the one-dimensional Ising Model in 1925 which was given to him as a problem by his advisor Wilhelm Lenz (below). Ising solved the model and indicated that no phase transitions occured between particles which would later be corrected for higher dimensions. Although Ising solved the one-dimensional model, the two dimensional square-lattice model is much harder to solve and was later looked at analytically by Lars Onsager in 1944. The two-dimensional can now be solved using transfer matrix method which is one of the many techniques used for solved lattices alongside quantum theory.  

<p align="center"><img src="/images/ising.png" height="200" width="200" />   <img src="/images/lenz.jfif" height="200" width="200" /></p>  
<p align="center"><em>Ernst Ising (1924), Wilhelm Lenz (1920)</em></p>  

One of the reasons why Ising model is famous for its lattice structure is that it lays out the interaction between its particles in an energy-based manner. Particles acquire spin values which allows them to interact with adjacent particles in the model, hence impacting the overall configuration of teh system. Eventually, particle-particle (or rather spin-spin) interactions end up minimiznig the energy of the system which results in a thermal equilibrium. Additionally, the model temperature can be annealed in simlutaion to excite the particles in a more drastic manner and assess their behavior which would eventually result in thermal equilibrium. Thermal equilibrium is a desirable from a computational perspective as it guarantees convergence in an algorithm. For instance, consider an algorithm which was constructed to solve the Ising model and does not have any knowledge of its particle positions. The algorithm will begin by tuning the temperature and particle spins in order to make the model converge. Once the model is converged, the problem is considered to be solved, indicating that the problem has atleast one solution. The equilibrium property of Ising model assures modern-day programmers that the problem can be solved and efficient solutions to approximations may exist.  

However, existence of a solution does not serve as a valid motviation for the usage of Ising model since there are multiple frameworks which guarantee the existence of many solutions in algorithms. The primary reason for the usage of Ising model is its structure. Ising model constitutes of particles arranged in a random configuration which rearrange themselves to yield optimal energy. Thi can be directly translated to modern-day neural networks which consist of particles (neurons) that adjust their weights (shift their positions in the lattice) to yield an optimal solution (energy). The deep structural connection between Ising lattices and computational graphs served as one of the major motivations for their adoption into the machine learning community back in the early 1970s. A number of different inference frameworks such as the Boltzmann Machine (BM), Restricted Boltzmann Machine (RBM) and Hopfield Network (HN) utilized energy-based objectives which were motivated by Ising computations. Additionally, the HN served as one of foundational examples of memory-based learning leveraging the interaction between neurons which is analogous to determining optimal spin-spin configurations in the lattice.  

<p align="center"><img src="/images/struct.jfif" height="200" width="200" />   <img src="/images/rbm.png" height="200" width="300" />   <img src="/images/hn.png" height="200" width="200" /></p>  
<p align="center"><em>Structural similarity between 2D Ising Model (left), RBM (center) and HN (right) highlighting the widespread adoption of the lattice framework</em></p>  

<h3>But what exactly is an Ising Model?</h3>


<h3>Math, math, math ...</h3>


<h3>Generalizing lattices in $n$-dimensions</h3>


<h3>Curtain</h3>



