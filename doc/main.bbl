\begin{thebibliography}{10}

\bibitem{lifelong}
D.~Abel, D.~Arumugam, L.~Lehnert, and M.~Littman.
\newblock State abstractions for lifelong reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{program}
D.~Andre and S.~J. Russell.
\newblock State abstraction for programmable reinforcement learning agents.
\newblock In {\em AAAI/IAAI}, 2002.

\bibitem{rbm}
K.-I. Aoki and T.~Kobayashi.
\newblock Restricted boltzmann machines for the long range ising models.
\newblock {\em Modern Physics Letters B}, 2016.

\bibitem{option}
P.-L. Bacon, J.~Harb, and D.~Precup.
\newblock The option-critic architecture, 2016.

\bibitem{smirl}
G.~Berseth, D.~Geng, C.~Devin, D.~Jayaraman, C.~Finn, and S.~Levine.
\newblock Smirl: Surprise minimizing rl in entropic environments.
\newblock 2019.

\bibitem{gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym, 2016.

\bibitem{gen}
J.~Z. Chen.
\newblock Reinforcement learning generalization with surprise minimization,
  2020.

\bibitem{intro}
B.~A. Cipra.
\newblock An introduction to the ising model.
\newblock {\em The American Mathematical Monthly}, 1987.

\bibitem{self}
J.~D. Co-Reyes, Y.~Liu, A.~Gupta, B.~Eysenbach, P.~Abbeel, and S.~Levine.
\newblock Self-consistent trajectory autoencoder: Hierarchical reinforcement
  learning with trajectory embeddings.
\newblock {\em arXiv preprint arXiv:1806.02813}, 2018.

\bibitem{spin}
G.~Delfino and G.~Mussardo.
\newblock The spin-spin correlation function in the two-dimensional ising model
  in a magnetic field at t= tc.
\newblock {\em Nuclear Physics B}, 1995.

\bibitem{baselines}
P.~Dhariwal, C.~Hesse, O.~Klimov, A.~Nichol, M.~Plappert, A.~Radford,
  J.~Schulman, S.~Sidor, Y.~Wu, and P.~Zhokhov.
\newblock Openai baselines, 2017.

\bibitem{maxq}
T.~G. Dietterich.
\newblock State abstraction in maxq hierarchical reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2000.

\bibitem{inverse}
C.~Finn, P.~Christiano, P.~Abbeel, and S.~Levine.
\newblock A connection between generative adversarial networks, inverse
  reinforcement learning, and energy-based models, 2016.

\bibitem{stochastic}
C.~Florensa, Y.~Duan, and P.~Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning,
  2017.

\bibitem{coma}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson.
\newblock Counterfactual multi-agent policy gradients, 2017.

\bibitem{td3}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em CoRR}, abs/1802.09477, 2018.

\bibitem{lsp}
T.~Haarnoja, K.~Hartikainen, P.~Abbeel, and S.~Levine.
\newblock Latent space policies for hierarchical reinforcement learning, 2018.

\bibitem{sql}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies, 2017.

\bibitem{sac}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor, 2018.

\bibitem{dreamer}
D.~Hafner, T.~Lillicrap, J.~Ba, and M.~Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock {\em arXiv preprint arXiv:1912.01603}, 2019.

\bibitem{book}
L.~N. Hand and J.~D. Finch.
\newblock {\em Analytical Mechanics}.
\newblock Cambridge University Press, 1998.

\bibitem{termination}
A.~Harutyunyan, W.~Dabney, D.~Borsa, N.~Heess, R.~Munos, and D.~Precup.
\newblock The termination critic, 2019.

\bibitem{ac}
N.~Heess, D.~Silver, and Y.~W. Teh.
\newblock Actor-critic reinforcement learning with energy-based policies.
\newblock Proceedings of Machine Learning Research, 2013.

\bibitem{fast}
H.~Herrmann.
\newblock Fast algorithm for the simulation of ising models.
\newblock {\em Journal of statistical physics}, 1986.

\bibitem{rainbow}
M.~Hessel, J.~Modayil, H.~Van~Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1710.02298}, 2017.

\bibitem{convergence}
J.~Honorio.
\newblock Convergence rates of biased stochastic optimization for learning
  sparse ising models, 2012.

\bibitem{ising-energy}
N.~Ito.
\newblock Non-equilibrium relaxation and interface energy of the ising model.
\newblock {\em Physica A: Statistical Mechanics and its Applications}, 1993.

\bibitem{quadruped}
D.~Jain, A.~Iscen, and K.~Caluwaerts.
\newblock Hierarchical reinforcement learning for quadruped locomotion.
\newblock {\em 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, 2019.

\bibitem{language}
Y.~Jiang, S.~S. Gu, K.~P. Murphy, and C.~Finn.
\newblock Language as an abstraction for hierarchical deep reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems 32}. 2019.

\bibitem{utility}
N.~K. Jong, T.~Hester, and P.~Stone.
\newblock The utility of temporal abstraction in reinforcement learning.
\newblock In {\em AAMAS (1)}, 2008.

\bibitem{scaling}
L.~Kadanoff.
\newblock {Scaling laws for Ising models near T(c)}.
\newblock 1966.

\bibitem{kostrikov}
I.~Kostrikov.
\newblock Pytorch implementations of reinforcement learning algorithms, 2018.

\bibitem{hdqn}
T.~D. Kulkarni, K.~R. Narasimhan, A.~Saeedi, and J.~B. Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation, 2016.

\bibitem{dsn}
T.~D. Kulkarni, A.~Saeedi, S.~Gautam, and S.~J. Gershman.
\newblock Deep successor reinforcement learning, 2016.

\bibitem{rad}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data, 2020.

\bibitem{energy}
Y.~LeCun, S.~Chopra, R.~Hadsell, F.~J. Huang, and et~al.
\newblock A tutorial on energy-based learning.
\newblock In {\em PREDICTING STRUCTURED DATA}, 2006.

\bibitem{hippo}
A.~Li, C.~Florensa, I.~Clavera, and P.~Abbeel.
\newblock Sub-policy adaptation for hierarchical reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{soac}
C.~Li, X.~Ma, C.~Zhang, J.~Yang, L.~Xia, and Q.~Zhao.
\newblock Soac: The soft option actor-critic architecture, 2020.

\bibitem{ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~M.~O. Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em CoRR}, abs/1509.02971, 2015.

\bibitem{evolutionary}
S.~Lin and R.~Wright.
\newblock Evolutionary tile coding: An automated state abstraction algorithm
  for reinforcement learning.
\newblock In {\em Proceedings of the 8th AAAI Conference on Abstraction,
  Reformulation, and Approximation}, 2010.

\bibitem{learn-param}
A.~Y. Lokhov, M.~Vuffray, S.~Misra, and M.~Chertkov.
\newblock Optimal structure and parameter learning of ising models.
\newblock {\em Science advances}, 2018.

\bibitem{combinatorial}
Q.~Ma, S.~Ge, D.~He, D.~Thaker, and I.~Drori.
\newblock Combinatorial optimization by graph pointer networks and hierarchical
  reinforcement learning, 2019.

\bibitem{david}
D.~J.~C. MacKay.
\newblock {\em Information Theory, Inference \& Learning Algorithms}.
\newblock Cambridge University Press, 2002.

\bibitem{maven}
A.~Mahajan, T.~Rashid, M.~Samvelyan, and S.~Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock In {\em Advances in Neural Information Processing Systems 32}. 2019.

\bibitem{a2c}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, 2016.

\bibitem{learn-critic}
A.~Morningstar and R.~G. Melko.
\newblock Deep learning the ising model near criticality.
\newblock {\em J. Mach. Learn. Res.}, 2017.

\bibitem{energy-hier}
O.~Nachum, S.~Gu, H.~Lee, and S.~Levine.
\newblock Near-optimal representation learning for hierarchical reinforcement
  learning, 2018.

\bibitem{relu}
V.~Nair and G.~E. Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em ICML}, 2010.

\bibitem{advantage}
T.~Osa, V.~Tangkaratt, and M.~Sugiyama.
\newblock Hierarchical reinforcement learning via advantage-weighted
  information maximization, 2019.

\bibitem{mpc}
A.~Pashevich, D.~Hafner, J.~Davidson, R.~Sukthankar, and C.~Schmid.
\newblock Modulated policy hierarchies, 2018.

\bibitem{control}
X.~B. Peng, M.~Chang, G.~Zhang, P.~Abbeel, and S.~Levine.
\newblock Mcp: Learning composable hierarchical control with multiplicative
  compositional policies, 2019.

\bibitem{lipschitz}
A.~Pensia, V.~Jog, and P.-L. Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In {\em 2018 IEEE International Symposium on Information Theory
  (ISIT)}, 2018.

\bibitem{temp}
D.~Precup.
\newblock Temporal abstraction in reinforcement learning, 2000.

\bibitem{qmix}
T.~Rashid, M.~Samvelyan, C.~S. de~Witt, G.~Farquhar, J.~Foerster, and
  S.~Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em ICML 2018: Proceedings of the Thirty-Fifth International
  Conference on Machine Learning}, 2018.

\bibitem{vb}
T.~Salimans, I.~Goodfellow, W.~Zaremba, V.~Cheung, A.~Radford, and X.~Chen.
\newblock Improved techniques for training gans.
\newblock In {\em Advances in neural information processing systems}, 2016.

\bibitem{es}
T.~Salimans, J.~Ho, X.~Chen, S.~Sidor, and I.~Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning, 2017.

\bibitem{energy-rl}
B.~Sallans and G.~E. Hinton.
\newblock Reinforcement learning with factored states and actions.
\newblock {\em The Journal of Machine Learning Research}, 2004.

\bibitem{smac}
M.~Samvelyan, T.~Rashid, C.~S. de~Witt, G.~Farquhar, N.~Nardelli, T.~G.~J.
  Rudner, C.-M. Hung, P.~H.~S. Torr, J.~Foerster, and S.~Whiteson.
\newblock The starcraft multi-agent challenge, 2019.

\bibitem{exact}
N.~N. Schraudolph and D.~Kamenetsky.
\newblock Efficient exact inference in planar ising models.
\newblock In {\em Advances in Neural Information Processing Systems 21}. 2009.

\bibitem{ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em CoRR}, abs/1707.06347, 2017.

\bibitem{diversity}
Y.~Song, J.~Wang, T.~Lukasiewicz, Z.~Xu, and M.~Xu.
\newblock Diversity-driven extensible hierarchical reinforcement learning.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  33, 2019.

\bibitem{curl}
A.~Srinivas, M.~Laskin, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning, 2020.

\bibitem{embedding}
S.~Sukhbaatar, E.~Denton, A.~Szlam, and R.~Fergus.
\newblock Learning goal embeddings via self-play for hierarchical reinforcement
  learning, 2018.

\bibitem{vdn}
P.~Sunehag, G.~Lever, A.~Gruslys, W.~M. Czarnecki, V.~Zambaldi, M.~Jaderberg,
  M.~Lanctot, N.~Sonnerat, J.~Z. Leibo, K.~Tuyls, and T.~Graepel.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS ’18, page 2085–2087,
  2018.

\bibitem{esac}
K.~Suri, X.~Shi, K.~Plataniotis, and Y.~Lawryshyn.
\newblock Evolve to control: Evolution-based soft actor-critic for scalable
  reinforcement learning.
\newblock {\em arXiv preprint}, 2020.

\bibitem{emix}
K.~Suri, X.~Q. Shi, K.~Plataniotis, and Y.~Lawryshyn.
\newblock Energy-based surprise minimization for multi-agent value
  factorization.
\newblock {\em arXiv preprint arXiv:2009.09842}, 2020.

\bibitem{sutton}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock 2018.

\bibitem{iql}
M.~Tan.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In {\em In Proceedings of the Tenth International Conference on
  Machine Learning}, 1993.

\bibitem{dm}
Y.~Tassa, Y.~Doron, A.~Muldal, T.~Erez, Y.~Li, D.~de~Las~Casas, D.~Budden,
  A.~Abdolmaleki, J.~Merel, A.~Lefrancq, T.~Lillicrap, and M.~Riedmiller.
\newblock Deepmind control suite, 2018.

\bibitem{overcomplete}
Y.~W. Teh, M.~Welling, S.~Osindero, and G.~E. Hinton.
\newblock Energy-based models for sparse overcomplete representations.
\newblock {\em The Journal of Machine Learning Research}, 2003.

\bibitem{thompson}
C.~J. Thompson.
\newblock {\em Mathematical Statistical Mechanics}.
\newblock Princeton University Press, 1972.

\bibitem{kl}
D.~Tirumala, H.~Noh, A.~Galashov, L.~Hasenclever, A.~Ahuja, G.~Wayne,
  R.~Pascanu, Y.~W. Teh, and N.~Heess.
\newblock Exploiting hierarchy for learning and transfer in kl-regularized rl,
  2019.

\bibitem{real}
G.~Tkacik, E.~Schneidman, M.~J.~B. II, and W.~Bialek.
\newblock Ising models for networks of real neurons, 2006.

\bibitem{mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock IROS, 2012.

\bibitem{learningtemp}
W.~Wang, Y.~Hu, and S.~Scherer.
\newblock Learning temporal abstraction with information-theoretic constraints
  for hierarchical reinforcement learning, 2020.

\bibitem{stats}
G.~H. Wannier.
\newblock Statistical physics.
\newblock 1967.

\bibitem{field}
T.~T. Wu, B.~M. McCoy, C.~A. Tracy, and E.~Barouch.
\newblock {Spin spin correlation functions for the two-dimensional Ising model:
  Exact theory in the scaling region}.
\newblock {\em Phys. Rev. B}, 1976.

\bibitem{acktr}
Y.~Wu, E.~Mansimov, R.~B. Grosse, S.~Liao, and J.~Ba.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In {\em Advances in neural information processing systems}, 2017.

\bibitem{compositional}
M.~Wulfmeier, A.~Abdolmaleki, R.~Hafner, J.~Tobias~Springenberg, M.~Neunert,
  N.~Siegel, T.~Hertweck, T.~Lampe, N.~Heess, and M.~Riedmiller.
\newblock Compositional transfer in hierarchical reinforcement learning.
\newblock {\em Robotics: Science and Systems XVI}, 2020.

\bibitem{sacae}
D.~Yarats, A.~Zhang, I.~Kostrikov, B.~Amos, J.~Pineau, and R.~Fergus.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock {\em arXiv preprint arXiv:1910.01741}, 2019.

\bibitem{transform}
N.~Yoshioka, Y.~Akagi, and H.~Katsura.
\newblock Transforming generalized ising models into boltzmann machines.
\newblock {\em Physical Review E}, 2019.

\bibitem{doubleoption}
S.~Zhang and S.~Whiteson.
\newblock Dac: The double actor-critic architecture for learning options, 2019.

\bibitem{mazelab}
X.~Zuo.
\newblock mazelab: A customizable framework to create maze and gridworld
  environments., 2018.

\end{thebibliography}
